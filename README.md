# ğŸ§  Chat LLM Local com llama.cpp e WebSocket

Este projeto Ã© uma aplicaÃ§Ã£o de **chat local** usando **modelos de linguagem (LLMs)** executados 100% **offline**, com interface Web e backend em Python via **WebSocket**. Ele utiliza a biblioteca `llama.cpp` para rodar modelos compactos no formato GGUF, ideais para uso local, sem necessidade de GPU.

---

## âœ¨ Funcionalidades

- Servidor Python com WebSocket
- Suporte a modelos `.gguf` via `llama-cpp-python`
- Download automÃ¡tico do modelo na primeira execuÃ§Ã£o
- Cliente Web com interface tipo chat (balÃµes Ã  esquerda/direita)
- BotÃµes de conectar e desconectar
- 100% offline

---

### Estrutura

chat-llm-local/
â”‚
â”œâ”€â”€ models/ # Onde o modelo .gguf serÃ¡ baixado
â”‚
â”œâ”€â”€ client.html # Interface web (chat)
â”œâ”€â”€ server.py # Servidor Python com WebSocket + LLM
â”œâ”€â”€ client.py # Client Python para terminal
â””â”€â”€ README.md # Este arquivo

---

## ğŸ› ï¸ Tecnologias e DependÃªncias

### Backend (Python)

- `llama-cpp-python` â€” interface Python para `llama.cpp`
- `websockets` â€” servidor WebSocket
- `requests` â€” download do modelo
- `tqdm` â€” barra de progresso

Instale com:

```bash
pip install llama-cpp-python websockets requests tqdm

or

pip install -r requirements.txt

```

### Executando o Servidor

```bash
python server.py

```

### ğŸ“Œ ObservaÃ§Ãµes

A performance depende do seu hardware (CPU ou GPU compatÃ­vel com AVX2 ou Metal)
Modelos maiores tambÃ©m sÃ£o suportados, mas exigem mais RAM
VocÃª pode customizar o prompt, histÃ³rico, estilos e streaming conforme desejar
